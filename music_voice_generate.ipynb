{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import scipy\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def generate_music(music_prompt):\n",
    "    \"\"\"\n",
    "    Generate music based on a given prompt and save it as a WAV and MP3 file.\n",
    "\n",
    "    Parameters:\n",
    "        music_prompt (str): The prompt for generating music.\n",
    "\n",
    "    This function initializes a processor and a model from the 'facebook/musicgen-small' pre-trained model. It prepares the inputs, generates audio, saves it as a WAV file, converts it to MP3, and then deletes the WAV file.\n",
    "\n",
    "    The generated music is saved in the current directory as 'musicgen_out.mp3'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the processor and model\n",
    "    processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "    model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n",
    "\n",
    "    # Prepare inputs\n",
    "    inputs = processor(\n",
    "        text=[f'{music_prompt}'],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Generate audio\n",
    "    audio_values = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "    # Save as WAV file\n",
    "    sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "    scipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=audio_values[0, 0].numpy())\n",
    "\n",
    "    # Convert WAV to MP3\n",
    "    wav_audio = AudioSegment.from_wav(\"musicgen_out.wav\")\n",
    "    wav_audio.export(\"musicgen_out.mp3\", format=\"mp3\")\n",
    "\n",
    "    # Delete the WAV file\n",
    "    os.remove(\"musicgen_out.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt being sent to the language model\n",
      "Instructions: Give a description for music that can be used for a given situation\n",
      "\n",
      "Input: 80s Coffee shop\n",
      "Output: 80s pop track with bassy drums and synth, heavy drumming, pop music, rock music, electric.\n",
      "\n",
      "\n",
      "Input: {video_prompt}\n",
      "Output:\n",
      "response from language model\n",
      " Car chase music is typically fast-paced and high-energy to mimic the intensity of a high-speed pursuit. It can be anything from dubstep to electronic dance music to house music. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akshi\\Desktop\\MIT\\pcv project\\code\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Cohere\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "def generate_music_descriptions(video_prompt):\n",
    "    \"\"\"\n",
    "    Generate music descriptions based on a given video prompt using a language model and template.\n",
    "\n",
    "    Parameters:\n",
    "        video_prompt (str): The prompt for generating music descriptions.\n",
    "\n",
    "    This function creates a template for providing music descriptions for given situations, fills in the template with the video prompt, and then sends the template to a language model for generating music descriptions. It prints the generated response from the language model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create prompt template\n",
    "    template = \"\"\"Instructions: Give a description for music that can be used for a given situation\\n\\nInput: 80s Coffee shop\\nOutput: 80s pop track with bassy drums and synth, heavy drumming, pop music, rock music, electric.\\n\\n\\nInput: {video_prompt}\\nOutput:\"\"\"\n",
    "    print(\"Prompt being sent to the language model\")\n",
    "    print(template)\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"video_prompt\"])\n",
    "\n",
    "    llm = Cohere(cohere_api_key=os.getenv(\"COHERE_API_KEY\") , stop=['\\n\\n\\n'], temperature=0.5, model='command')\n",
    "\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    response = llm_chain.run(video_prompt)\n",
    "    print(\"response from language model\")\n",
    "    print(response)\n",
    "\n",
    "music_description = generate_music_descriptions(\"Car Chase\")\n",
    "generate_music(music_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "def voice_generation(dialogue, gender, output_audio_destination_path):\n",
    "    \"\"\"\n",
    "    Generate voice using the Elevenlabs Text-to-Speech API and save the output audio to a destination path.\n",
    "\n",
    "    Parameters:\n",
    "        dialogue (str): The text to be converted into speech.\n",
    "        gender (str): The gender of the voice (either 'female' or 'male').\n",
    "        output_audio_destination_path (str): The path where the generated audio will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create Voice using Elevenlabs\n",
    "    CHUNK_SIZE = 1024\n",
    "    id = \"\"\n",
    "    if gender == \"female\":\n",
    "        id = \"21m00Tcm4TlvDq8ikWAM\"\n",
    "    elif gender == \"male\":\n",
    "        id = \"ODq5zmih8GrVes37Dizd\"\n",
    "\n",
    "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{id}\"\n",
    "\n",
    "    headers = {\n",
    "    \"Accept\": \"audio/mpeg\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"xi-api-key\": os.getenv(\"ELEVENLABS_API_KEY\")\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "    \"text\": dialogue,\n",
    "    \"model_id\": \"eleven_monolingual_v1\",\n",
    "    \"voice_settings\": {\n",
    "        \"stability\": 0.5,\n",
    "        \"similarity_boost\": 0.5\n",
    "    }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    with open('output.mp3', 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "    # Copy the generated audio to the unreal engine project\n",
    "    shutil.copy('output.mp3', output_audio_destination_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
